# Stacking-for-house-price
基于随机采样与精度加权的Stacking算法，用于kaggle竞赛House Price的房价预测，比传统stacking算法有所提升

# 摘要
近年来，人工智能的强势崛起让我们领略到人工智能技术的巨大潜力，机器学习也被广泛应用于各个领域，并取得不错的成果。本文以Kaggle竞赛House Prices的房价数据为实验样本，借鉴Bagging的自助采样法和k折交叉验证法，构建一种基于伪随机采样的Stacking集成模型，用于房价预测。首先利用GBDT对数据集进行简单训练，并得到各个特征重要性。接着对数据集进行多次随机采样，然后根据特征重要性进行属性扰动，组成多个训练数据子集和验证数据子集。用这些数据子集训练基模型，并计算验证集的均方根误差和预测结果，根据误差分配权重。根据各个基模型预测结果组成第二层的元模型，最后在测试数据集上进行房价预测。实验结果表明，基于随机采样和精度加权的Stacking集成模型的均方根误差小于所有基分类器和同结构的经典Stacking集成方法。
# Stacking算法理论基础

  Stacking是一种分层模型集成框架，在1992年被Wolpert提出。Stacking集成可以有多层的情况，但通常会设计两层，第一层由多种基模型组成，输入为原始训练集，而输出为各种基模型的预测值，而第二层只有一个元模型，对第一层的各种模型的预测值和真实值进行训练，从而得到完成的集成模型。同理，预测测试集的过程也要先经过所有基模型的预测，组成第二层的特征，再用第二层的元模型预测出最终的结果。为了防止模型过拟合的情况，一般Stacking算法在第一层训练基模型时会结合k折交叉验证法。以五折交叉验证法为例，Stacking算法的过程如下图所示。
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210602223645181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lvdV9qdXN0X2xvb2s=,size_16,color_FFFFFF,t_70#pic_center)



# Stacking改进
## 改进思路

本文对传统的Stacking算法进行研究改进，以Kaggle竞赛House Prices的房价数据为实验样本，通过Kaggle测试得分验证改进方法的可行性。本文以以下的方式对传统Stacking算法进行改进：

1. 使用无放回抽样得到数据子集。目前传统的Stacking算法是采用五折交叉验证法，将训练数据分成5等份，依次选择其中一份作为验证子集，而其他四份作为训练子集用于模型训练，用训练好的基模型去预测验证子集，预测结果作为第二层的特征。而本文的模型则选择随机不放回抽样，比如连续20次随机抽取80%的样本，组成20个独立的数据子集。

2. 根据概率随机选取特征。数据集经过数据处理和特征工厂后会产生很多特征，尤其是对离散特征进行独热编码，使得特征空间会变得非常大，而且存在很多冗余特征。因此本文利用GBDT对数据集进行简单训练，并得到各个特征重要性，组成总和为1的概率列表。利用这个概率列表随机选取特征，可过滤冗余特征，构造效率更高、消耗更低的独立的预测模型。

3. 根据训练集的测试精度进行测试集的权重分配。传统的Stacking算法是采用五折交叉验证法，将数据集划分成五等份，由五组数据子集构成5个基模型，在第一层预测测试集时，取5个基模型的预测结果的平均值作为第二层的特征。这里可能存在数据划分不均，而导致预测效果不佳的情况。因此本文根据基模型的测试精度对预测结果进行加权平均，得到结果作为第二层的特征。

## 改进Stacking代码

> subsample函数是对数据集进行样本与特征的采样，并记录采样情况，因为预测的时候需要对测试集的特征进行相同的采样。
> .
> 改进的算法有三个超参数：
> 1. n_tree：基模型个数T
> 2. ratio_sample：样本采样比例a
> 3. ratio_feature：特征采样比例b
> 
> .
> 首先先用GBoost进行简单训练，得到特征重要性列表
> .
> 在stacking框架第一层，对每一个基模型进行T次拷贝，根据样本采样比例a对样本进行T次随机采样，然后再根据特征采样比例b和特征重要性列表进行特征选择，得到T个训练子集和T个验证子集。用训练子集分别对基模型进行训练，然后将对相应的验证子集的预测结果作为第二层元模型的输入特征。同时根据验证集的预测值与真实值的误差，给T个基模型分配权重。误差越大，权重越低。

权重计算公式：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210602231607137.png)




# 结果

|模型  |均方根误差  |
|--|--|
| 岭回归 | 0.13666 |
| LASSO回归 | 0.13181|
| 弹性网络回归 | 0.13174 |
| 梯度提升树 | 0.13278 |
| 传统stacking集成模型 | 0.12254 |
| 改进stacking集成模型 | 0.12060 |




# 引用
本文数据来源于Kaggle竞赛House Prices的房价数据，测试结果通过Kaggle竞赛上传数据得到。
本次数据处理主要源于https://my.oschina.net/Kanonpy/blog/3076731

 [1]:  House Prices - Advanced Regression Techniques[EB/OL]. https://www.kaggle.com/c/house-prices-advanced-regression-techniques,2016-8-30.
 [2]: https://my.oschina.net/Kanonpy/blog/3076731
 [3]: 鲁莹, 郑少智.Stacking 学习与一般集成方法的比较研究[D].暨南大学,2017.
 [4]: 覃智全. Stacking集成分类器优化算法研究[D].国防科学技术大学,2016.
 [5]: 张笑铭,王志君,梁利平.一种适用于卷积神经网络的Stacking算法[J].计算机工程,2018,44(04):243-247.
 [6]: 徐慧丽. Stacking算法的研究及改进[D].华南理工大学,2018.
 [7]: 陈宇韶. 基于特征选择与改进Stacking算法的股价预测研究[D].南华大学,2018.
